{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SubClassifier_script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashwal-B-S/PlantDiseaseDetection/blob/main/SubClassifier_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1048PD4mL9"
      },
      "source": [
        "Getting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQd93BontXw",
        "outputId": "5bad70b5-1406-4a9c-e631-58c371376045"
      },
      "source": [
        "!git clone https://github.com/deepakHonakeri05/yolo_dataset.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolo_dataset'...\n",
            "remote: Enumerating objects: 36553, done.\u001b[K\n",
            "remote: Total 36553 (delta 0), reused 0 (delta 0), pack-reused 36553\u001b[K\n",
            "Receiving objects: 100% (36553/36553), 632.83 MiB | 19.42 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Checking out files: 100% (72348/72348), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sm1MY7XnXUf",
        "outputId": "bd49be3b-ad25-4e57-c133-ce8c14fc8cd1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCOI2GlQL0Qk"
      },
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.python.keras.models import Sequential,load_model,Model\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwEenC6hLV1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f4531a-90a2-4cc6-8a99-9ecce9a17bb1"
      },
      "source": [
        "# PEACHES\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "\n",
        "# model.add(Dense(1024, activation = DENSE_LAYER_ACTIVATION))\n",
        "# 2nd layer as Dense for 2-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n",
        "\n",
        "# model.load_weights('./peach.h5')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8P0vqvCL53B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b52754-41ef-481e-aa0e-137ce97eab00"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRXn2dxJTmFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29446ad2-c93a-4a12-b298-d6f7b1b584f5"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Peach/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Peach/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=20,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=9)\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/peachweights.h5\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2126 images belonging to 2 classes.\n",
            "Found 531 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "34/34 [==============================] - 52s 1s/step - loss: 0.4073 - accuracy: 0.8650 - val_loss: 0.1305 - val_accuracy: 0.9642\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 33s 972ms/step - loss: 0.2558 - accuracy: 0.9177 - val_loss: 0.0802 - val_accuracy: 0.9755\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 34s 996ms/step - loss: 0.2510 - accuracy: 0.9163 - val_loss: 0.0617 - val_accuracy: 0.9849\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 33s 975ms/step - loss: 0.2422 - accuracy: 0.9205 - val_loss: 0.0477 - val_accuracy: 0.9868\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 34s 998ms/step - loss: 0.2365 - accuracy: 0.9186 - val_loss: 0.0462 - val_accuracy: 0.9906\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 33s 972ms/step - loss: 0.2372 - accuracy: 0.9196 - val_loss: 0.0478 - val_accuracy: 0.9831\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 34s 989ms/step - loss: 0.2326 - accuracy: 0.9200 - val_loss: 0.0443 - val_accuracy: 0.9868\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 33s 973ms/step - loss: 0.2585 - accuracy: 0.9120 - val_loss: 0.0660 - val_accuracy: 0.9774\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 34s 1s/step - loss: 0.2255 - accuracy: 0.9243 - val_loss: 0.0330 - val_accuracy: 0.9906\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 34s 997ms/step - loss: 0.2221 - accuracy: 0.9238 - val_loss: 0.0320 - val_accuracy: 0.9962\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 34s 999ms/step - loss: 0.2267 - accuracy: 0.9205 - val_loss: 0.0312 - val_accuracy: 0.9906\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 34s 997ms/step - loss: 0.2246 - accuracy: 0.9257 - val_loss: 0.0237 - val_accuracy: 0.9962\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 34s 994ms/step - loss: 0.2210 - accuracy: 0.9262 - val_loss: 0.0378 - val_accuracy: 0.9868\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 33s 981ms/step - loss: 0.2167 - accuracy: 0.9271 - val_loss: 0.0306 - val_accuracy: 0.9925\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 33s 1s/step - loss: 0.2204 - accuracy: 0.9247 - val_loss: 0.0272 - val_accuracy: 0.9906\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 33s 976ms/step - loss: 0.2284 - accuracy: 0.9200 - val_loss: 0.0379 - val_accuracy: 0.9887\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 33s 984ms/step - loss: 0.2148 - accuracy: 0.9285 - val_loss: 0.0313 - val_accuracy: 0.9944\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 34s 998ms/step - loss: 0.2248 - accuracy: 0.9233 - val_loss: 0.0178 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 33s 972ms/step - loss: 0.1968 - accuracy: 0.9370 - val_loss: 0.0202 - val_accuracy: 0.9944\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 33s 988ms/step - loss: 0.2112 - accuracy: 0.9276 - val_loss: 0.0199 - val_accuracy: 0.9906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-TiESblhbxZ"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62BWdcIpmjTM"
      },
      "source": [
        "# GRAPE\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 4-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67U85dwPUJ5L",
        "outputId": "473baf7b-5c08-4434-adb4-7eade6370165"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 8196      \n",
            "=================================================================\n",
            "Total params: 23,595,908\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgFqR7BHUJ8S",
        "outputId": "66b03fd0-85d3-49a3-96a4-59614cb0fb06"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Grape/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Grape/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=25,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=13)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/grapeweights.h5\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3251 images belonging to 4 classes.\n",
            "Found 811 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "51/51 [==============================] - 56s 1s/step - loss: 1.1493 - accuracy: 0.4740 - val_loss: 0.3941 - val_accuracy: 0.8779\n",
            "Epoch 2/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.8836 - accuracy: 0.5798 - val_loss: 0.2583 - val_accuracy: 0.9248\n",
            "Epoch 3/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.8372 - accuracy: 0.6020 - val_loss: 0.1958 - val_accuracy: 0.9433\n",
            "Epoch 4/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.8309 - accuracy: 0.5915 - val_loss: 0.1708 - val_accuracy: 0.9445\n",
            "Epoch 5/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7963 - accuracy: 0.6173 - val_loss: 0.1566 - val_accuracy: 0.9544\n",
            "Epoch 6/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7849 - accuracy: 0.6189 - val_loss: 0.1493 - val_accuracy: 0.9433\n",
            "Epoch 7/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7541 - accuracy: 0.6373 - val_loss: 0.1339 - val_accuracy: 0.9482\n",
            "Epoch 8/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7794 - accuracy: 0.6112 - val_loss: 0.1306 - val_accuracy: 0.9556\n",
            "Epoch 9/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7580 - accuracy: 0.6201 - val_loss: 0.1020 - val_accuracy: 0.9581\n",
            "Epoch 10/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7718 - accuracy: 0.6217 - val_loss: 0.0938 - val_accuracy: 0.9692\n",
            "Epoch 11/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7537 - accuracy: 0.6297 - val_loss: 0.0988 - val_accuracy: 0.9679\n",
            "Epoch 12/25\n",
            "51/51 [==============================] - 51s 1s/step - loss: 0.7509 - accuracy: 0.6386 - val_loss: 0.1114 - val_accuracy: 0.9618\n",
            "Epoch 13/25\n",
            "51/51 [==============================] - 51s 1s/step - loss: 0.7256 - accuracy: 0.6456 - val_loss: 0.1071 - val_accuracy: 0.9655\n",
            "Epoch 14/25\n",
            "51/51 [==============================] - 53s 1s/step - loss: 0.7312 - accuracy: 0.6395 - val_loss: 0.1011 - val_accuracy: 0.9593\n",
            "Epoch 15/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7424 - accuracy: 0.6269 - val_loss: 0.1013 - val_accuracy: 0.9618\n",
            "Epoch 16/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7260 - accuracy: 0.6429 - val_loss: 0.0918 - val_accuracy: 0.9667\n",
            "Epoch 17/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7387 - accuracy: 0.6281 - val_loss: 0.0791 - val_accuracy: 0.9729\n",
            "Epoch 18/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7410 - accuracy: 0.6358 - val_loss: 0.0956 - val_accuracy: 0.9593\n",
            "Epoch 19/25\n",
            "51/51 [==============================] - 51s 1s/step - loss: 0.7442 - accuracy: 0.6223 - val_loss: 0.0755 - val_accuracy: 0.9704\n",
            "Epoch 20/25\n",
            "51/51 [==============================] - 51s 1s/step - loss: 0.7558 - accuracy: 0.6238 - val_loss: 0.0948 - val_accuracy: 0.9642\n",
            "Epoch 21/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7516 - accuracy: 0.6247 - val_loss: 0.0887 - val_accuracy: 0.9642\n",
            "Epoch 22/25\n",
            "51/51 [==============================] - 53s 1s/step - loss: 0.7315 - accuracy: 0.6349 - val_loss: 0.0772 - val_accuracy: 0.9766\n",
            "Epoch 23/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7451 - accuracy: 0.6290 - val_loss: 0.0717 - val_accuracy: 0.9766\n",
            "Epoch 24/25\n",
            "51/51 [==============================] - 53s 1s/step - loss: 0.7215 - accuracy: 0.6376 - val_loss: 0.0624 - val_accuracy: 0.9790\n",
            "Epoch 25/25\n",
            "51/51 [==============================] - 52s 1s/step - loss: 0.7341 - accuracy: 0.6358 - val_loss: 0.0813 - val_accuracy: 0.9741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRP6FeDpXNJo"
      },
      "source": [
        "# APPLE\n",
        "\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 4-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n",
        "\n",
        "\n",
        "# model.load_weights('./apple.h5')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtdwVV7BXNJp",
        "outputId": "2303cacf-d209-4267-b687-79312d70d1f9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 8196      \n",
            "=================================================================\n",
            "Total params: 23,595,908\n",
            "Trainable params: 8,196\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHUUqZ7wXNJp",
        "outputId": "cbbd001d-f4bf-4229-cb0d-41d35c031c4d"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Apple/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/Apple/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=25,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=10)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/appleweights.h5\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2537 images belonging to 4 classes.\n",
            "Found 634 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "40/40 [==============================] - 46s 1s/step - loss: 0.9860 - accuracy: 0.6165 - val_loss: 0.4184 - val_accuracy: 0.8707\n",
            "Epoch 2/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.7941 - accuracy: 0.7024 - val_loss: 0.2367 - val_accuracy: 0.9464\n",
            "Epoch 3/25\n",
            "40/40 [==============================] - 42s 1s/step - loss: 0.7667 - accuracy: 0.7123 - val_loss: 0.2016 - val_accuracy: 0.9511\n",
            "Epoch 4/25\n",
            "40/40 [==============================] - 40s 992ms/step - loss: 0.7526 - accuracy: 0.7130 - val_loss: 0.1680 - val_accuracy: 0.9653\n",
            "Epoch 5/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.7511 - accuracy: 0.7028 - val_loss: 0.1499 - val_accuracy: 0.9637\n",
            "Epoch 6/25\n",
            "40/40 [==============================] - 40s 995ms/step - loss: 0.7371 - accuracy: 0.7103 - val_loss: 0.1199 - val_accuracy: 0.9621\n",
            "Epoch 7/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.7340 - accuracy: 0.7154 - val_loss: 0.1353 - val_accuracy: 0.9621\n",
            "Epoch 8/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6975 - accuracy: 0.7312 - val_loss: 0.1028 - val_accuracy: 0.9763\n",
            "Epoch 9/25\n",
            "40/40 [==============================] - 39s 987ms/step - loss: 0.6812 - accuracy: 0.7367 - val_loss: 0.1100 - val_accuracy: 0.9637\n",
            "Epoch 10/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.7020 - accuracy: 0.7268 - val_loss: 0.0840 - val_accuracy: 0.9826\n",
            "Epoch 11/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6911 - accuracy: 0.7264 - val_loss: 0.0836 - val_accuracy: 0.9842\n",
            "Epoch 12/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6746 - accuracy: 0.7335 - val_loss: 0.0734 - val_accuracy: 0.9842\n",
            "Epoch 13/25\n",
            "40/40 [==============================] - 40s 989ms/step - loss: 0.6795 - accuracy: 0.7347 - val_loss: 0.0760 - val_accuracy: 0.9795\n",
            "Epoch 14/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6637 - accuracy: 0.7410 - val_loss: 0.0823 - val_accuracy: 0.9700\n",
            "Epoch 15/25\n",
            "40/40 [==============================] - 40s 999ms/step - loss: 0.6748 - accuracy: 0.7383 - val_loss: 0.0771 - val_accuracy: 0.9732\n",
            "Epoch 16/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6643 - accuracy: 0.7371 - val_loss: 0.0744 - val_accuracy: 0.9811\n",
            "Epoch 17/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6509 - accuracy: 0.7446 - val_loss: 0.0605 - val_accuracy: 0.9779\n",
            "Epoch 18/25\n",
            "40/40 [==============================] - 39s 987ms/step - loss: 0.6702 - accuracy: 0.7316 - val_loss: 0.0720 - val_accuracy: 0.9779\n",
            "Epoch 19/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6766 - accuracy: 0.7331 - val_loss: 0.0926 - val_accuracy: 0.9779\n",
            "Epoch 20/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6662 - accuracy: 0.7399 - val_loss: 0.0682 - val_accuracy: 0.9763\n",
            "Epoch 21/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6547 - accuracy: 0.7418 - val_loss: 0.0998 - val_accuracy: 0.9637\n",
            "Epoch 22/25\n",
            "40/40 [==============================] - 40s 992ms/step - loss: 0.6539 - accuracy: 0.7477 - val_loss: 0.0696 - val_accuracy: 0.9763\n",
            "Epoch 23/25\n",
            "40/40 [==============================] - 41s 1s/step - loss: 0.6716 - accuracy: 0.7363 - val_loss: 0.0698 - val_accuracy: 0.9748\n",
            "Epoch 24/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6773 - accuracy: 0.7324 - val_loss: 0.0665 - val_accuracy: 0.9811\n",
            "Epoch 25/25\n",
            "40/40 [==============================] - 40s 1s/step - loss: 0.6625 - accuracy: 0.7379 - val_loss: 0.0583 - val_accuracy: 0.9795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6CIzAklYsMm"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8korbkmYsM3"
      },
      "source": [
        "# CHERRY\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 2-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n",
        "\n",
        "\n",
        "# model.load_weights('./apple.h5')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylmq2NJsYsM4",
        "outputId": "b7becd26-2676-41c0-b425-d50e7f274237"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4LZWtk3YsM4",
        "outputId": "e7ea400b-cbb6-4c9c-f7d8-3384f71f4fc6"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/cherry/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/cherry/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=20,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=6)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/cherryweights.h5\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1374 images belonging to 2 classes.\n",
            "Found 380 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "22/22 [==============================] - 27s 1s/step - loss: 0.5508 - accuracy: 0.6776 - val_loss: 0.1038 - val_accuracy: 0.9789\n",
            "Epoch 2/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3867 - accuracy: 0.7656 - val_loss: 0.0488 - val_accuracy: 0.9895\n",
            "Epoch 3/20\n",
            "22/22 [==============================] - 24s 1s/step - loss: 0.3787 - accuracy: 0.7533 - val_loss: 0.0309 - val_accuracy: 0.9974\n",
            "Epoch 4/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3719 - accuracy: 0.7809 - val_loss: 0.0243 - val_accuracy: 0.9974\n",
            "Epoch 5/20\n",
            "22/22 [==============================] - 22s 996ms/step - loss: 0.3781 - accuracy: 0.7627 - val_loss: 0.0186 - val_accuracy: 0.9974\n",
            "Epoch 6/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3657 - accuracy: 0.7635 - val_loss: 0.0164 - val_accuracy: 0.9974\n",
            "Epoch 7/20\n",
            "22/22 [==============================] - 22s 995ms/step - loss: 0.3691 - accuracy: 0.7686 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "22/22 [==============================] - 22s 987ms/step - loss: 0.3791 - accuracy: 0.7591 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3658 - accuracy: 0.7569 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3601 - accuracy: 0.7569 - val_loss: 0.0099 - val_accuracy: 0.9974\n",
            "Epoch 11/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3628 - accuracy: 0.7751 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3728 - accuracy: 0.7620 - val_loss: 0.0100 - val_accuracy: 0.9947\n",
            "Epoch 13/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3578 - accuracy: 0.7678 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3559 - accuracy: 0.7576 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3512 - accuracy: 0.7511 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3578 - accuracy: 0.7562 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "22/22 [==============================] - 22s 983ms/step - loss: 0.3508 - accuracy: 0.7642 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "22/22 [==============================] - 22s 1s/step - loss: 0.3920 - accuracy: 0.7285 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "22/22 [==============================] - 23s 1s/step - loss: 0.3578 - accuracy: 0.7438 - val_loss: 0.0059 - val_accuracy: 0.9974\n",
            "Epoch 20/20\n",
            "22/22 [==============================] - 22s 986ms/step - loss: 0.3622 - accuracy: 0.7584 - val_loss: 0.0037 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wKUYdggZo_M"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsFrcOEZZo_N"
      },
      "source": [
        "# PEPPER\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 2-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGWBtxO8Zo_O",
        "outputId": "088c775e-2739-4fc7-dee8-be21da40d610"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgwjw11cZo_O",
        "outputId": "bc6fc589-5ccf-4986-d2dd-cec7b853d3b9"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/pepper/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/pepper/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=20,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=8)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/pepperweights.h5\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1784 images belonging to 2 classes.\n",
            "Found 494 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "28/28 [==============================] - 33s 1s/step - loss: 0.5919 - accuracy: 0.6592 - val_loss: 0.2647 - val_accuracy: 0.9089\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4567 - accuracy: 0.7506 - val_loss: 0.1817 - val_accuracy: 0.9474\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4482 - accuracy: 0.7646 - val_loss: 0.1785 - val_accuracy: 0.9534\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4458 - accuracy: 0.7522 - val_loss: 0.1289 - val_accuracy: 0.9676\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4438 - accuracy: 0.7422 - val_loss: 0.1292 - val_accuracy: 0.9737\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4358 - accuracy: 0.7595 - val_loss: 0.1204 - val_accuracy: 0.9737\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 28s 1s/step - loss: 0.4334 - accuracy: 0.7506 - val_loss: 0.1199 - val_accuracy: 0.9818\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4344 - accuracy: 0.7635 - val_loss: 0.1069 - val_accuracy: 0.9838\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 28s 1s/step - loss: 0.4365 - accuracy: 0.7584 - val_loss: 0.1143 - val_accuracy: 0.9757\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4126 - accuracy: 0.7567 - val_loss: 0.1466 - val_accuracy: 0.9696\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4040 - accuracy: 0.7590 - val_loss: 0.1186 - val_accuracy: 0.9798\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4120 - accuracy: 0.7377 - val_loss: 0.1155 - val_accuracy: 0.9696\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4012 - accuracy: 0.7567 - val_loss: 0.1219 - val_accuracy: 0.9717\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 28s 1s/step - loss: 0.4183 - accuracy: 0.7539 - val_loss: 0.1110 - val_accuracy: 0.9798\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4042 - accuracy: 0.7685 - val_loss: 0.0858 - val_accuracy: 0.9858\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.3940 - accuracy: 0.7791 - val_loss: 0.0919 - val_accuracy: 0.9879\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 28s 1s/step - loss: 0.3976 - accuracy: 0.7668 - val_loss: 0.1227 - val_accuracy: 0.9777\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4060 - accuracy: 0.7646 - val_loss: 0.1093 - val_accuracy: 0.9879\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3930 - accuracy: 0.7786 - val_loss: 0.0995 - val_accuracy: 0.9798\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3880 - accuracy: 0.7803 - val_loss: 0.0896 - val_accuracy: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl1P3D7radw2"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4vJPvSyadxD"
      },
      "source": [
        "# POTATO\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 3-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n",
        "\n",
        "\n",
        "# model.load_weights('./apple.h5')\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUp55Uo_adxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7270b9-b790-4d47-ba56-97b0674c8f40"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 6147      \n",
            "=================================================================\n",
            "Total params: 23,593,859\n",
            "Trainable params: 6,147\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUjOiQo2adxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2e9b60-2e93-4536-c324-590729ec143e"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/potato/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/potato/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=25,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=7)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/potatoweights.h5\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1722 images belonging to 3 classes.\n",
            "Found 430 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "27/27 [==============================] - 32s 1s/step - loss: 0.8681 - accuracy: 0.5528 - val_loss: 0.5257 - val_accuracy: 0.7860\n",
            "Epoch 2/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.6871 - accuracy: 0.6463 - val_loss: 0.3013 - val_accuracy: 0.8814\n",
            "Epoch 3/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.6216 - accuracy: 0.6858 - val_loss: 0.2275 - val_accuracy: 0.9186\n",
            "Epoch 4/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.6252 - accuracy: 0.6585 - val_loss: 0.2378 - val_accuracy: 0.9140\n",
            "Epoch 5/25\n",
            "27/27 [==============================] - 29s 1s/step - loss: 0.6122 - accuracy: 0.6603 - val_loss: 0.2262 - val_accuracy: 0.9233\n",
            "Epoch 6/25\n",
            "27/27 [==============================] - 29s 1s/step - loss: 0.5981 - accuracy: 0.6777 - val_loss: 0.1874 - val_accuracy: 0.9279\n",
            "Epoch 7/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5771 - accuracy: 0.6934 - val_loss: 0.1692 - val_accuracy: 0.9581\n",
            "Epoch 8/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.5917 - accuracy: 0.6864 - val_loss: 0.1528 - val_accuracy: 0.9535\n",
            "Epoch 9/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5701 - accuracy: 0.6864 - val_loss: 0.1529 - val_accuracy: 0.9395\n",
            "Epoch 10/25\n",
            "27/27 [==============================] - 27s 998ms/step - loss: 0.5711 - accuracy: 0.7021 - val_loss: 0.1421 - val_accuracy: 0.9558\n",
            "Epoch 11/25\n",
            "27/27 [==============================] - 29s 1s/step - loss: 0.5563 - accuracy: 0.6852 - val_loss: 0.1387 - val_accuracy: 0.9581\n",
            "Epoch 12/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.5592 - accuracy: 0.6852 - val_loss: 0.1050 - val_accuracy: 0.9744\n",
            "Epoch 13/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5452 - accuracy: 0.7033 - val_loss: 0.1090 - val_accuracy: 0.9698\n",
            "Epoch 14/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5524 - accuracy: 0.6882 - val_loss: 0.1292 - val_accuracy: 0.9512\n",
            "Epoch 15/25\n",
            "27/27 [==============================] - 27s 997ms/step - loss: 0.5655 - accuracy: 0.6794 - val_loss: 0.0931 - val_accuracy: 0.9651\n",
            "Epoch 16/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5317 - accuracy: 0.7125 - val_loss: 0.1131 - val_accuracy: 0.9674\n",
            "Epoch 17/25\n",
            "27/27 [==============================] - 29s 1s/step - loss: 0.5238 - accuracy: 0.7172 - val_loss: 0.1013 - val_accuracy: 0.9628\n",
            "Epoch 18/25\n",
            "27/27 [==============================] - 27s 994ms/step - loss: 0.5482 - accuracy: 0.6922 - val_loss: 0.1318 - val_accuracy: 0.9605\n",
            "Epoch 19/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5381 - accuracy: 0.7003 - val_loss: 0.1090 - val_accuracy: 0.9605\n",
            "Epoch 20/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5243 - accuracy: 0.7033 - val_loss: 0.0881 - val_accuracy: 0.9744\n",
            "Epoch 21/25\n",
            "27/27 [==============================] - 27s 995ms/step - loss: 0.5346 - accuracy: 0.7085 - val_loss: 0.0849 - val_accuracy: 0.9744\n",
            "Epoch 22/25\n",
            "27/27 [==============================] - 29s 1s/step - loss: 0.5404 - accuracy: 0.6980 - val_loss: 0.0844 - val_accuracy: 0.9721\n",
            "Epoch 23/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.5137 - accuracy: 0.7184 - val_loss: 0.0804 - val_accuracy: 0.9744\n",
            "Epoch 24/25\n",
            "27/27 [==============================] - 28s 1s/step - loss: 0.5386 - accuracy: 0.6998 - val_loss: 0.1327 - val_accuracy: 0.9465\n",
            "Epoch 25/25\n",
            "27/27 [==============================] - 27s 1s/step - loss: 0.5274 - accuracy: 0.7050 - val_loss: 0.0885 - val_accuracy: 0.9698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0hIw96Oa3iC"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yTj2dZKa3iS"
      },
      "source": [
        "# STRAWBERRY\n",
        "\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 2-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIPwimgMa3iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ccaded-ead6-40ca-d4a7-9ce2c41f67db"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTksmUBJa3iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f85ffd-342e-458a-8f0f-d69d24875652"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/strawberry/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/strawberry/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=20,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=5)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/strawberryweights.h5\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1129 images belonging to 2 classes.\n",
            "Found 312 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "18/18 [==============================] - 22s 1s/step - loss: 0.5049 - accuracy: 0.7741 - val_loss: 0.1105 - val_accuracy: 0.9744\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3520 - accuracy: 0.8397 - val_loss: 0.0480 - val_accuracy: 0.9936\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3372 - accuracy: 0.8450 - val_loss: 0.0371 - val_accuracy: 0.9968\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3307 - accuracy: 0.8450 - val_loss: 0.0289 - val_accuracy: 0.9936\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3317 - accuracy: 0.8432 - val_loss: 0.0269 - val_accuracy: 0.9968\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3369 - accuracy: 0.8441 - val_loss: 0.0207 - val_accuracy: 0.9968\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3209 - accuracy: 0.8468 - val_loss: 0.0190 - val_accuracy: 0.9936\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3339 - accuracy: 0.8423 - val_loss: 0.0172 - val_accuracy: 0.9968\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3329 - accuracy: 0.8397 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3212 - accuracy: 0.8530 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3271 - accuracy: 0.8459 - val_loss: 0.0075 - val_accuracy: 0.9968\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3213 - accuracy: 0.8494 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3131 - accuracy: 0.8609 - val_loss: 0.0087 - val_accuracy: 0.9968\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3330 - accuracy: 0.8485 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3138 - accuracy: 0.8485 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3228 - accuracy: 0.8512 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3056 - accuracy: 0.8601 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 18s 995ms/step - loss: 0.3241 - accuracy: 0.8494 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 18s 1s/step - loss: 0.3052 - accuracy: 0.8565 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 19s 1s/step - loss: 0.3050 - accuracy: 0.8618 - val_loss: 0.0045 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k26YDSTbU0R"
      },
      "source": [
        "Rebuilding the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KSdfT7JbU0S"
      },
      "source": [
        "# TOMATO\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "RESNET50_POOLING_AVERAGE = 'avg'\n",
        "DENSE_LAYER_ACTIVATION = 'softmax'\n",
        "OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n",
        "\n",
        "# Common accuracy metric for all outputs, but can use different metrics for different output\n",
        "LOSS_METRICS = ['accuracy']\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st layer as the lumpsum weights from resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# NOTE that this layer will be set below as NOT TRAINABLE, i.e., use it as is\n",
        "model.add(ResNet50(include_top = False, pooling = RESNET50_POOLING_AVERAGE, weights = 'imagenet'))\n",
        "\n",
        "#model.add(Dense(512, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# 2nd layer as Dense for 10-class classification\n",
        "model.add(Dense(NUM_CLASSES, activation = DENSE_LAYER_ACTIVATION))\n",
        "\n",
        "# Say not to train first layer (ResNet) model as it is already trained\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "#compiling\n",
        "model.compile(optimizer = 'adam', loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dDP9NVnbU0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9a76a3-58c3-4673-a0fe-a3a76d1eb361"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,608,202\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkY1kbxpbU0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab6b222-0443-4b3e-af2f-fa674f38e8d8"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 270,\n",
        "    brightness_range = (-2,2),\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "    vertical_flip=True,\n",
        "    validation_split=0.3,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "val_datagen = ImageDataGenerator(vertical_flip=True,\n",
        "                                 horizontal_flip=True,\n",
        "                                 rotation_range=180,\n",
        "                                 zoom_range=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        'yolo_dataset/tomato/train/',\n",
        "        target_size=(224,224),\n",
        "        batch_size=64,\n",
        "        seed=101, \n",
        "        shuffle=True,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        'yolo_dataset/tomato/test/',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        seed=101,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# val_set = val_datagen.flow_from_directory(\n",
        "#         'yolo_dataset/pepper/val/',\n",
        "#         target_size=(224, 224),\n",
        "#         batch_size=64,\n",
        "#         shuffle=True,\n",
        "#         seed=101,\n",
        "#         class_mode='categorical')\n",
        "\n",
        "history = model.fit_generator(\n",
        "        train_set,\n",
        "        epochs=15,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=57)\n",
        "\n",
        "\n",
        "model.save_weights(\"/content/drive/My Drive/Leaf_Weights/Weights/tomatoweights.h5\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14531 images belonging to 10 classes.\n",
            "Found 3628 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "228/228 [==============================] - 233s 1s/step - loss: 1.6128 - accuracy: 0.4642 - val_loss: 0.6060 - val_accuracy: 0.8054\n",
            "Epoch 2/15\n",
            "228/228 [==============================] - 229s 1s/step - loss: 1.4006 - accuracy: 0.5313 - val_loss: 0.4430 - val_accuracy: 0.8597\n",
            "Epoch 3/15\n",
            "228/228 [==============================] - 229s 1s/step - loss: 1.3234 - accuracy: 0.5600 - val_loss: 0.3965 - val_accuracy: 0.8735\n",
            "Epoch 4/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.3098 - accuracy: 0.5623 - val_loss: 0.3659 - val_accuracy: 0.8842\n",
            "Epoch 5/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.3071 - accuracy: 0.5599 - val_loss: 0.3489 - val_accuracy: 0.8840\n",
            "Epoch 6/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.2938 - accuracy: 0.5684 - val_loss: 0.3164 - val_accuracy: 0.9027\n",
            "Epoch 7/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.2824 - accuracy: 0.5718 - val_loss: 0.3404 - val_accuracy: 0.8886\n",
            "Epoch 8/15\n",
            "228/228 [==============================] - 227s 996ms/step - loss: 1.2788 - accuracy: 0.5744 - val_loss: 0.2836 - val_accuracy: 0.9063\n",
            "Epoch 9/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.2671 - accuracy: 0.5768 - val_loss: 0.2841 - val_accuracy: 0.9096\n",
            "Epoch 10/15\n",
            "228/228 [==============================] - 228s 1s/step - loss: 1.2571 - accuracy: 0.5821 - val_loss: 0.2621 - val_accuracy: 0.9140\n",
            "Epoch 11/15\n",
            "228/228 [==============================] - 226s 993ms/step - loss: 1.2621 - accuracy: 0.5796 - val_loss: 0.2555 - val_accuracy: 0.9148\n",
            "Epoch 12/15\n",
            "228/228 [==============================] - 229s 1s/step - loss: 1.2580 - accuracy: 0.5812 - val_loss: 0.2662 - val_accuracy: 0.9115\n",
            "Epoch 13/15\n",
            "228/228 [==============================] - 228s 999ms/step - loss: 1.2630 - accuracy: 0.5780 - val_loss: 0.2680 - val_accuracy: 0.9129\n",
            "Epoch 14/15\n",
            "228/228 [==============================] - 228s 1000ms/step - loss: 1.2445 - accuracy: 0.5852 - val_loss: 0.2739 - val_accuracy: 0.9132\n",
            "Epoch 15/15\n",
            "228/228 [==============================] - 228s 999ms/step - loss: 1.2415 - accuracy: 0.5862 - val_loss: 0.2416 - val_accuracy: 0.9259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM41_y1ExGTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7535621-6cc5-4436-f611-79a224b2be3e"
      },
      "source": [
        "scores = model.evaluate_generator(generator=test_set)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1973: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 92.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSeM6Z6pvCx8",
        "outputId": "758bdd50-124e-4e39-d671-0d300364d9eb"
      },
      "source": [
        "scores"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.23910745978355408, 0.9211686849594116]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvhDbefVvKM9",
        "outputId": "839439ce-b829-4635-9fb3-fcd9258ff869"
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'accuracy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEQ8vG-1vxdT",
        "outputId": "1d32f043-dce3-4113-cdda-55c0bef8e7bb"
      },
      "source": [
        "history.history"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.4642488360404968,\n",
              "  0.5313467979431152,\n",
              "  0.5599752068519592,\n",
              "  0.562315046787262,\n",
              "  0.5599064230918884,\n",
              "  0.5684399008750916,\n",
              "  0.5718119740486145,\n",
              "  0.5743582844734192,\n",
              "  0.5767669081687927,\n",
              "  0.5820659399032593,\n",
              "  0.5795884728431702,\n",
              "  0.5812401175498962,\n",
              "  0.5780056715011597,\n",
              "  0.5851627588272095,\n",
              "  0.5861950516700745],\n",
              " 'loss': [1.6128222942352295,\n",
              "  1.4005721807479858,\n",
              "  1.3233790397644043,\n",
              "  1.3098342418670654,\n",
              "  1.3070740699768066,\n",
              "  1.2938200235366821,\n",
              "  1.2823655605316162,\n",
              "  1.2787569761276245,\n",
              "  1.2671267986297607,\n",
              "  1.2571214437484741,\n",
              "  1.2621090412139893,\n",
              "  1.2580336332321167,\n",
              "  1.2629936933517456,\n",
              "  1.2444589138031006,\n",
              "  1.2414859533309937],\n",
              " 'val_accuracy': [0.805402398109436,\n",
              "  0.8597022891044617,\n",
              "  0.8734840154647827,\n",
              "  0.8842337131500244,\n",
              "  0.883958101272583,\n",
              "  0.902701199054718,\n",
              "  0.8886438608169556,\n",
              "  0.9062844514846802,\n",
              "  0.9095920324325562,\n",
              "  0.9140021800994873,\n",
              "  0.9148291349411011,\n",
              "  0.9115214943885803,\n",
              "  0.9128996729850769,\n",
              "  0.9131752848625183,\n",
              "  0.9258544445037842],\n",
              " 'val_loss': [0.6059871912002563,\n",
              "  0.4430210590362549,\n",
              "  0.3965394198894501,\n",
              "  0.36593812704086304,\n",
              "  0.34890589118003845,\n",
              "  0.31641682982444763,\n",
              "  0.3404493033885956,\n",
              "  0.283555805683136,\n",
              "  0.2840811014175415,\n",
              "  0.2620806396007538,\n",
              "  0.2555235028266907,\n",
              "  0.2662336826324463,\n",
              "  0.2680135667324066,\n",
              "  0.27391523122787476,\n",
              "  0.24155868589878082]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dj8uaf1h--d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "38c2929b-858e-449a-d219-f6d31e7e987f"
      },
      "source": [
        "plt.figure(1, figsize = (15,8)) \n",
        "    \n",
        "plt.subplot(221)  \n",
        "plt.plot(history.history['accuracy'])  \n",
        "plt.plot(history.history['val_accuracy'])  \n",
        "plt.title('Tomato model accuracy')  \n",
        "plt.ylabel('accuracy')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'valid']) \n",
        "    \n",
        "plt.subplot(222)  \n",
        "plt.plot(history.history['loss'])  \n",
        "plt.plot(history.history['val_loss'])  \n",
        "plt.title('Tomato model loss')  \n",
        "plt.ylabel('loss')  \n",
        "plt.xlabel('epoch')  \n",
        "plt.legend(['train', 'valid']) \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}